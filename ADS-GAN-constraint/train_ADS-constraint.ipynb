{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os, sys, time\n",
    "sys.path.append(\"..\")\n",
    "from all_funcs import util\n",
    "from model import Generator, Discriminator, train_discriminator, train_generator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"1\"\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../dataset/df_noOutliner_ana.csv\",index_col=0)\n",
    "df, imp_mode, imp_mean=util.FeatureArrange(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reduce redundant features which can be assembled\n",
    "dataset=df.drop(['NIHTotal','THD_ID','cortical_CT', 'subcortical_CT',\n",
    "              'circulation_CT', 'CT_find', 'watershed_CT', 'Hemorrhagic_infarct_CT',\n",
    "              'CT_left', 'CT_right',],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare for inverse tensor values from range(0,1) to original values\n",
    "params=dict()\n",
    "params['max']=dataset.max().to_numpy()\n",
    "params['min']=dataset.min().to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "dataset.loc[:,dataset.columns] = sc.fit_transform(dataset.loc[:,dataset.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting hyperparameter\n",
    "latent_dim = dataset.shape[1]\n",
    "epochs = 45000\n",
    "batch_size= 128\n",
    "buffer_size = 6000\n",
    "# save_interval = 50\n",
    "n_critic = 5\n",
    "checkpoint_dir = './training_checkpoints'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(latent_dim)\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create Cross Entropy\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = tf.keras.optimizers.Adam(0.00001,0.5)\n",
    "disc_opt = tf.keras.optimizers.Adam(0.00001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save checkpoints\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "# checkpoint = tf.train.Checkpoint(generator_optimizer=gen_opt,\n",
    "#                                  discriminator_optimizer=disc_opt,\n",
    "#                                  generator=generator,\n",
    "#                                  discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(dataset, test_size=0.2,shuffle=True,\n",
    "                                   stratify=dataset['elapsed_class'],\n",
    "                                   random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer generator is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Time for epoch 1 is 1.1880743503570557 sec - gen_loss = 80.04693209120832, disc_loss = 209.1294482862701\n",
      "Time for epoch 101 is 0.048003196716308594 sec - gen_loss = 28.885881412174967, disc_loss = -0.01309195507163754\n",
      "Time for epoch 201 is 0.05200338363647461 sec - gen_loss = 25.784504057852875, disc_loss = -0.0030633807170431246\n",
      "Time for epoch 301 is 0.05200338363647461 sec - gen_loss = 21.550308074894225, disc_loss = -0.00282466797369935\n",
      "Time for epoch 401 is 0.05200338363647461 sec - gen_loss = 17.5547534331892, disc_loss = -0.0023544772900658973\n",
      "Time for epoch 501 is 0.05200314521789551 sec - gen_loss = 14.618397938336102, disc_loss = -0.0013013401150149275\n",
      "Time for epoch 601 is 0.048003196716308594 sec - gen_loss = 12.419448233423113, disc_loss = -0.0018106419320348688\n",
      "Time for epoch 701 is 0.05200338363647461 sec - gen_loss = 10.68717383017, disc_loss = -0.0015017342968046685\n",
      "Time for epoch 801 is 0.04800295829772949 sec - gen_loss = 9.40304572460308, disc_loss = -0.0015551068255148267\n",
      "Time for epoch 901 is 0.06000375747680664 sec - gen_loss = 8.16259346300553, disc_loss = -0.0015076109503045984\n",
      "Time for epoch 1001 is 0.05200314521789551 sec - gen_loss = 7.229145616269603, disc_loss = -0.000978009886180368\n",
      "Time for epoch 1101 is 0.052002906799316406 sec - gen_loss = 6.382072913192034, disc_loss = -0.0009416426394759401\n",
      "Time for epoch 1201 is 0.06400370597839355 sec - gen_loss = 5.924369164766023, disc_loss = -0.0009206130273085669\n",
      "Time for epoch 1301 is 0.04800295829772949 sec - gen_loss = 5.389448799217814, disc_loss = -0.0007115148766378296\n",
      "Time for epoch 1401 is 0.05200338363647461 sec - gen_loss = 5.168489028166053, disc_loss = -0.0009748922736174042\n",
      "Time for epoch 1501 is 0.04800295829772949 sec - gen_loss = 4.758033227826138, disc_loss = -0.0009213292993498554\n",
      "Time for epoch 1601 is 0.056003570556640625 sec - gen_loss = 4.480589869781846, disc_loss = -0.0008924283624805485\n",
      "Time for epoch 1701 is 0.056003570556640625 sec - gen_loss = 4.234015793850957, disc_loss = -0.0008711336422349015\n",
      "Time for epoch 1801 is 0.06000375747680664 sec - gen_loss = 4.11641549867935, disc_loss = -0.000970375050138293\n",
      "Time for epoch 1901 is 0.04800271987915039 sec - gen_loss = 3.8894856378013634, disc_loss = -0.0009415726027338457\n",
      "Time for epoch 2001 is 0.056003570556640625 sec - gen_loss = 3.739914715305848, disc_loss = -0.0008728945425484841\n",
      "Time for epoch 2101 is 0.06400394439697266 sec - gen_loss = 3.6081363118233547, disc_loss = -0.0007967103611414858\n",
      "Time for epoch 2201 is 0.044002532958984375 sec - gen_loss = 3.5152740940978844, disc_loss = -0.000882575079047629\n",
      "Time for epoch 2301 is 0.06400418281555176 sec - gen_loss = 3.3723757691906697, disc_loss = -0.0007888121440651336\n",
      "Time for epoch 2401 is 0.056003570556640625 sec - gen_loss = 3.2831494149058895, disc_loss = -0.0009775123579736844\n",
      "Time for epoch 2501 is 0.06000351905822754 sec - gen_loss = 3.2330180742049768, disc_loss = -0.00101257786916051\n",
      "Time for epoch 2601 is 0.06000375747680664 sec - gen_loss = 3.1688920291608866, disc_loss = -0.000956069871854514\n",
      "Time for epoch 2701 is 0.056003570556640625 sec - gen_loss = 3.08083756715848, disc_loss = -0.0008682647381377422\n",
      "Time for epoch 2801 is 0.06000375747680664 sec - gen_loss = 3.0025069660597277, disc_loss = -0.0008781092995928889\n",
      "Time for epoch 2901 is 0.05200338363647461 sec - gen_loss = 2.978523860441348, disc_loss = -0.001121367334468692\n",
      "Time for epoch 3001 is 0.048003196716308594 sec - gen_loss = 2.9593423110276, disc_loss = -0.0008990682536918161\n",
      "Time for epoch 3101 is 0.06400418281555176 sec - gen_loss = 2.9205053361390534, disc_loss = -0.0008949933098140484\n",
      "Time for epoch 3201 is 0.056003570556640625 sec - gen_loss = 2.852149986883037, disc_loss = -0.0007743800905175389\n",
      "Time for epoch 3301 is 0.052002906799316406 sec - gen_loss = 2.843287116171367, disc_loss = -0.0008319357894252812\n",
      "Time for epoch 3401 is 0.05200314521789551 sec - gen_loss = 2.824919052831691, disc_loss = -0.0009534802877320569\n",
      "Time for epoch 3501 is 0.06000375747680664 sec - gen_loss = 2.772943109476904, disc_loss = -0.0010891871016564824\n",
      "Time for epoch 3601 is 0.06000351905822754 sec - gen_loss = 2.750744034104439, disc_loss = -0.001170953545844947\n",
      "Time for epoch 3701 is 0.06000375747680664 sec - gen_loss = 2.751716307531212, disc_loss = -0.0009153362506365434\n",
      "Time for epoch 3801 is 0.04800295829772949 sec - gen_loss = 2.7131361013403033, disc_loss = -0.0007446027468541174\n",
      "Time for epoch 3901 is 0.04800295829772949 sec - gen_loss = 2.659069843042344, disc_loss = -0.0009849277372202586\n",
      "Time for epoch 4001 is 0.04800295829772949 sec - gen_loss = 2.6245503815020204, disc_loss = -0.0007393812178267814\n",
      "Time for epoch 4101 is 0.06000375747680664 sec - gen_loss = 2.5832724508137845, disc_loss = -0.000987207092112748\n",
      "Time for epoch 4201 is 0.06000375747680664 sec - gen_loss = 2.597947132885928, disc_loss = -0.001153866055874647\n",
      "Time for epoch 4301 is 0.056003570556640625 sec - gen_loss = 2.575079819647096, disc_loss = -0.0010773903679023012\n",
      "Time for epoch 4401 is 0.04800295829772949 sec - gen_loss = 2.5568090952232714, disc_loss = -0.0011211998396634035\n",
      "Time for epoch 4501 is 0.056003570556640625 sec - gen_loss = 2.529094682700029, disc_loss = -0.0011495212127829806\n",
      "Time for epoch 4601 is 0.05600333213806152 sec - gen_loss = 2.5088841099309476, disc_loss = -0.0010824690516849907\n",
      "Time for epoch 4701 is 0.06000351905822754 sec - gen_loss = 2.5043403572163307, disc_loss = -0.0011408266353084124\n",
      "Time for epoch 4801 is 0.06000375747680664 sec - gen_loss = 2.4888461070781918, disc_loss = -0.0010749902240862234\n",
      "Time for epoch 4901 is 0.05200338363647461 sec - gen_loss = 2.4696343771960168, disc_loss = -0.0009120552395627975\n",
      "Time for epoch 5001 is 0.06400394439697266 sec - gen_loss = 2.469231240540241, disc_loss = -0.0010672925048881294\n",
      "Time for epoch 5101 is 0.06000399589538574 sec - gen_loss = 2.4348782162813047, disc_loss = -0.0010775449717345298\n",
      "Time for epoch 5201 is 0.06000351905822754 sec - gen_loss = 2.4313428534290273, disc_loss = -0.001061265408840667\n",
      "Time for epoch 5301 is 0.06800413131713867 sec - gen_loss = 2.4112614850558716, disc_loss = -0.0011476948195247405\n",
      "Time for epoch 5401 is 0.06000351905822754 sec - gen_loss = 2.41222744773429, disc_loss = -0.0009126340747799137\n",
      "Time for epoch 5501 is 0.06000375747680664 sec - gen_loss = 2.3958680791846136, disc_loss = -0.0011143160698889598\n",
      "Time for epoch 5601 is 0.05200314521789551 sec - gen_loss = 2.3847367491389564, disc_loss = -0.0009374888766485685\n",
      "Time for epoch 5701 is 0.06400370597839355 sec - gen_loss = 2.3774277513213833, disc_loss = -0.0008273744608894272\n",
      "Time for epoch 5801 is 0.056003570556640625 sec - gen_loss = 2.363637652852991, disc_loss = -0.0009027664164304721\n",
      "Time for epoch 5901 is 0.06400418281555176 sec - gen_loss = 2.361931996850559, disc_loss = -0.001002937536206673\n",
      "Time for epoch 6001 is 0.07200455665588379 sec - gen_loss = 2.3707423362920097, disc_loss = -0.0011463904387745338\n",
      "Time for epoch 6101 is 0.06400418281555176 sec - gen_loss = 2.340419578550703, disc_loss = -0.001014393154993811\n",
      "Time for epoch 6201 is 0.05200338363647461 sec - gen_loss = 2.340885047513094, disc_loss = -0.0009050873380636603\n",
      "Time for epoch 6301 is 0.05600333213806152 sec - gen_loss = 2.3251674935037894, disc_loss = -0.0009433544781192813\n",
      "Time for epoch 6401 is 0.05600333213806152 sec - gen_loss = 2.3091851245017834, disc_loss = -0.0009612699908348478\n",
      "Time for epoch 6501 is 0.06800413131713867 sec - gen_loss = 2.2953132899268125, disc_loss = -0.0008235990479811342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 6601 is 0.06400394439697266 sec - gen_loss = 2.2839437751517337, disc_loss = -0.0010307889376668657\n",
      "Time for epoch 6701 is 0.06800436973571777 sec - gen_loss = 2.2824760778295006, disc_loss = -0.0010378093798571648\n",
      "Time for epoch 6801 is 0.05200314521789551 sec - gen_loss = 2.2860250560595974, disc_loss = -0.0008702332818835834\n",
      "Time for epoch 6901 is 0.06000375747680664 sec - gen_loss = 2.275924899835657, disc_loss = -0.0007450152193916097\n",
      "Time for epoch 7001 is 0.06400418281555176 sec - gen_loss = 2.2436437157766544, disc_loss = -0.0009054162812921285\n",
      "Time for epoch 7101 is 0.06000375747680664 sec - gen_loss = 2.2446307174863533, disc_loss = -0.0006797145386501467\n",
      "Time for epoch 7201 is 0.056003570556640625 sec - gen_loss = 2.22950854962065, disc_loss = -0.0008182729672051676\n",
      "Time for epoch 7301 is 0.06400418281555176 sec - gen_loss = 2.2088322291388107, disc_loss = -0.0007064921282302587\n",
      "Time for epoch 7401 is 0.06800413131713867 sec - gen_loss = 2.18655452546843, disc_loss = -0.0007936633565058344\n",
      "Time for epoch 7501 is 0.06800413131713867 sec - gen_loss = 2.1850549761951914, disc_loss = -0.000726249352062819\n",
      "Time for epoch 7601 is 0.08000493049621582 sec - gen_loss = 2.1757130383286722, disc_loss = -0.0007482577957944864\n",
      "Time for epoch 7701 is 0.06400394439697266 sec - gen_loss = 2.1683709724396567, disc_loss = -0.000620492678046706\n",
      "Time for epoch 7801 is 0.07200479507446289 sec - gen_loss = 2.1512546552606486, disc_loss = -0.0007270756026696126\n",
      "Time for epoch 7901 is 0.06400418281555176 sec - gen_loss = 2.1495836020491375, disc_loss = -0.0007256893764688726\n",
      "Time for epoch 8001 is 0.06800436973571777 sec - gen_loss = 2.129582428459731, disc_loss = -0.0008233646912603898\n",
      "Time for epoch 8101 is 0.06800413131713867 sec - gen_loss = 2.1318833105002706, disc_loss = -0.0009974393855342403\n",
      "Time for epoch 8201 is 0.056003570556640625 sec - gen_loss = 2.1281103329448166, disc_loss = -0.0009672275858416751\n",
      "Time for epoch 8301 is 0.06800436973571777 sec - gen_loss = 2.1273571634260917, disc_loss = -0.0009010703826823568\n",
      "Time for epoch 8401 is 0.05200338363647461 sec - gen_loss = 2.1357980189642514, disc_loss = -0.0008207141005705857\n",
      "Time for epoch 8501 is 0.05600333213806152 sec - gen_loss = 2.108365448656978, disc_loss = -0.0006833625741679474\n",
      "Time for epoch 8601 is 0.06400394439697266 sec - gen_loss = 2.115414481201838, disc_loss = -0.0009307680712040276\n",
      "Time for epoch 8701 is 0.06800436973571777 sec - gen_loss = 2.107692167966647, disc_loss = -0.0007022381490653167\n",
      "Time for epoch 8801 is 0.06800413131713867 sec - gen_loss = 2.105423164539516, disc_loss = -0.0009633339277489432\n",
      "Time for epoch 8901 is 0.06800413131713867 sec - gen_loss = 2.099332675053987, disc_loss = -0.0010276279470004448\n",
      "Time for epoch 9001 is 0.08000493049621582 sec - gen_loss = 2.101692255637059, disc_loss = -0.0008918237686321261\n",
      "Time for epoch 9101 is 0.06800413131713867 sec - gen_loss = 2.078308304971959, disc_loss = -0.00042244884790621694\n",
      "Time for epoch 9201 is 0.07200455665588379 sec - gen_loss = 2.087128984140417, disc_loss = -0.0007754582233368784\n",
      "Time for epoch 9301 is 0.06800436973571777 sec - gen_loss = 2.0928024398897587, disc_loss = -0.0009245638240583063\n",
      "Time for epoch 9401 is 0.06800436973571777 sec - gen_loss = 2.084955436555977, disc_loss = -0.0007634747178431933\n",
      "Time for epoch 9501 is 0.0760045051574707 sec - gen_loss = 2.066914974525242, disc_loss = -0.0006475027090070432\n",
      "Time for epoch 9601 is 0.07200431823730469 sec - gen_loss = 2.0766898461320973, disc_loss = -0.0007018993229462123\n",
      "Time for epoch 9701 is 0.06400394439697266 sec - gen_loss = 2.059020393157543, disc_loss = -0.0006254256846341237\n",
      "Time for epoch 9801 is 0.08800530433654785 sec - gen_loss = 2.062093248176408, disc_loss = -0.000741125176601026\n",
      "Time for epoch 9901 is 0.06800436973571777 sec - gen_loss = 2.0658464885182966, disc_loss = -0.0006337118310224442\n",
      "Time for epoch 10001 is 0.06800413131713867 sec - gen_loss = 2.0587408137403482, disc_loss = -0.0006982344684673651\n",
      "Time for epoch 10101 is 0.06400370597839355 sec - gen_loss = 2.057985558038989, disc_loss = -0.0009087488748511618\n",
      "Time for epoch 10201 is 0.0760047435760498 sec - gen_loss = 2.0342614280263334, disc_loss = -0.000776071515075928\n",
      "Time for epoch 10301 is 0.08400535583496094 sec - gen_loss = 2.049610272208589, disc_loss = -0.0006513249829591882\n",
      "Time for epoch 10401 is 0.09600615501403809 sec - gen_loss = 2.0481151231856503, disc_loss = -0.0008698288569306033\n",
      "Time for epoch 10501 is 0.07200455665588379 sec - gen_loss = 2.044823130498002, disc_loss = -0.0007043759503107009\n",
      "Time for epoch 10601 is 0.08000516891479492 sec - gen_loss = 2.0464752631490666, disc_loss = -0.0005613476676418412\n",
      "Time for epoch 10701 is 0.08400559425354004 sec - gen_loss = 2.04044624965078, disc_loss = -0.0008647043759714314\n",
      "Time for epoch 10801 is 0.08400511741638184 sec - gen_loss = 2.0389690496858908, disc_loss = -0.0011766188086663242\n",
      "Time for epoch 10901 is 0.08000493049621582 sec - gen_loss = 2.0251338102028273, disc_loss = -0.0007303119113593909\n",
      "Time for epoch 11001 is 0.0760047435760498 sec - gen_loss = 2.022317474651082, disc_loss = -0.0008378272893694909\n",
      "Time for epoch 11101 is 0.07200455665588379 sec - gen_loss = 2.0255881510953, disc_loss = -0.0007589698468374906\n",
      "Time for epoch 11201 is 0.08400511741638184 sec - gen_loss = 2.014412388926862, disc_loss = -0.0005322752266382695\n",
      "Time for epoch 11301 is 0.0760049819946289 sec - gen_loss = 2.0163315264867583, disc_loss = -0.000595751206719718\n",
      "Time for epoch 11401 is 0.08400535583496094 sec - gen_loss = 2.016719907259009, disc_loss = -0.000683185586042867\n",
      "Time for epoch 11501 is 0.08000516891479492 sec - gen_loss = 2.0182238142545055, disc_loss = -0.0008414628585140041\n",
      "Time for epoch 11601 is 0.08400535583496094 sec - gen_loss = 2.0184340293367073, disc_loss = -0.0007897751944105037\n",
      "Time for epoch 11701 is 0.08000493049621582 sec - gen_loss = 2.004068390113178, disc_loss = -0.0007737089588467911\n",
      "Time for epoch 11801 is 0.08800530433654785 sec - gen_loss = 2.0087342630834115, disc_loss = -0.000684822070024627\n",
      "Time for epoch 11901 is 0.08400511741638184 sec - gen_loss = 2.011218265076133, disc_loss = -0.0007044317459904754\n",
      "Time for epoch 12001 is 0.09200572967529297 sec - gen_loss = 2.009319123702393, disc_loss = -0.0008498313724074521\n",
      "Time for epoch 12101 is 0.08000493049621582 sec - gen_loss = 1.995043426864109, disc_loss = -0.0009410107139847173\n",
      "Time for epoch 12201 is 0.08400535583496094 sec - gen_loss = 1.9969363185790523, disc_loss = -0.0005419456128217869\n",
      "Time for epoch 12301 is 0.08800554275512695 sec - gen_loss = 1.9937399932920756, disc_loss = -0.0005713913967509732\n",
      "Time for epoch 12401 is 0.09600567817687988 sec - gen_loss = 1.9865874761256528, disc_loss = -0.0006577188448749514\n",
      "Time for epoch 12501 is 0.100006103515625 sec - gen_loss = 1.9923088889451506, disc_loss = -0.0005373235877986309\n",
      "Time for epoch 12601 is 0.08400511741638184 sec - gen_loss = 1.9968383453465541, disc_loss = -0.0006538771941391478\n",
      "Time for epoch 12701 is 0.09200572967529297 sec - gen_loss = 1.9810813694396041, disc_loss = -0.000611884891598619\n",
      "Time for epoch 12801 is 0.1000063419342041 sec - gen_loss = 1.9739991516574764, disc_loss = -0.0006937867005612628\n",
      "Time for epoch 12901 is 0.09200549125671387 sec - gen_loss = 1.9766783043701892, disc_loss = -0.0004905354174049996\n",
      "Time for epoch 13001 is 0.08800554275512695 sec - gen_loss = 1.9757440599426235, disc_loss = -0.0007547975796809508\n",
      "Time for epoch 13101 is 0.09600591659545898 sec - gen_loss = 1.9724411718190955, disc_loss = -0.0005131312068644924\n",
      "Time for epoch 13201 is 0.09600567817687988 sec - gen_loss = 1.9604285740782335, disc_loss = -0.00038813136085460943\n",
      "Time for epoch 13301 is 0.08800530433654785 sec - gen_loss = 1.9646715410525872, disc_loss = -0.0005316213954935183\n",
      "Time for epoch 13401 is 0.10400652885437012 sec - gen_loss = 1.9615438924555626, disc_loss = -0.00043814390229099533\n",
      "Time for epoch 13501 is 0.10400676727294922 sec - gen_loss = 1.9638545800998994, disc_loss = -0.0006514496217616785\n",
      "Time for epoch 13601 is 0.10800695419311523 sec - gen_loss = 1.9573877509994746, disc_loss = -0.0004753005178590285\n",
      "Time for epoch 13701 is 0.1000063419342041 sec - gen_loss = 1.9575024625354664, disc_loss = -0.00047194364773749033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 13801 is 0.10400652885437012 sec - gen_loss = 1.948565335006292, disc_loss = -0.0006566402141271763\n",
      "Time for epoch 13901 is 0.10400629043579102 sec - gen_loss = 1.9519723154911968, disc_loss = -0.0006824841469727452\n",
      "Time for epoch 14001 is 0.09600615501403809 sec - gen_loss = 1.9622175956285781, disc_loss = -0.0006837332157806704\n",
      "Time for epoch 14101 is 0.100006103515625 sec - gen_loss = 1.9414321108912937, disc_loss = -0.0007983309196408222\n",
      "Time for epoch 14201 is 0.11200690269470215 sec - gen_loss = 1.9532336258601637, disc_loss = -0.0006572715982165082\n",
      "Time for epoch 14301 is 0.11600708961486816 sec - gen_loss = 1.954218688781152, disc_loss = -0.0005850233594647239\n",
      "Time for epoch 14401 is 0.10400652885437012 sec - gen_loss = 1.9493708042108722, disc_loss = -0.0008096039777970443\n",
      "Time for epoch 14501 is 0.11600708961486816 sec - gen_loss = 1.9472957620580653, disc_loss = -0.000734940399312186\n",
      "Time for epoch 14601 is 0.11200690269470215 sec - gen_loss = 1.9460518071868746, disc_loss = -0.0007490759003003519\n",
      "Time for epoch 14701 is 0.11200690269470215 sec - gen_loss = 1.9390911715549564, disc_loss = -0.0006939584957647015\n",
      "Time for epoch 14801 is 0.1280078887939453 sec - gen_loss = 1.9458546720552496, disc_loss = -0.0008358717709860705\n",
      "Time for epoch 14901 is 0.11200690269470215 sec - gen_loss = 1.941939144451475, disc_loss = -0.0007224633026416722\n",
      "Time for epoch 15001 is 0.1280078887939453 sec - gen_loss = 1.9322547181454266, disc_loss = -0.0007444530940994077\n",
      "Time for epoch 15101 is 0.11600756645202637 sec - gen_loss = 1.9398306718904035, disc_loss = -0.0007940336131847832\n",
      "Time for epoch 15201 is 0.14000892639160156 sec - gen_loss = 1.9374912752052416, disc_loss = -0.0007804571962069059\n",
      "Time for epoch 15301 is 0.12000727653503418 sec - gen_loss = 1.9305973591495653, disc_loss = -0.0008520041194643634\n",
      "Time for epoch 15401 is 0.14000892639160156 sec - gen_loss = 1.9203425363661868, disc_loss = -0.0010021648119632486\n",
      "Time for epoch 15501 is 0.16001009941101074 sec - gen_loss = 1.920038267342671, disc_loss = -0.000945955819887315\n",
      "Time for epoch 15601 is 0.14000868797302246 sec - gen_loss = 1.924447421327735, disc_loss = -0.0007421736928433463\n",
      "Time for epoch 15701 is 0.14400887489318848 sec - gen_loss = 1.9161970367135956, disc_loss = -0.0008667343747566187\n",
      "Time for epoch 15801 is 0.14400887489318848 sec - gen_loss = 1.9264273685076525, disc_loss = -0.0007115261650189892\n",
      "Time for epoch 15901 is 0.16401028633117676 sec - gen_loss = 1.9177358813595533, disc_loss = -0.0009291451774811356\n",
      "Time for epoch 16001 is 0.1520094871520996 sec - gen_loss = 1.9230214889336172, disc_loss = -0.0008568468688537184\n",
      "Time for epoch 16101 is 0.16401028633117676 sec - gen_loss = 1.9194419369990694, disc_loss = -0.0011642852275973437\n",
      "Time for epoch 16201 is 0.18001127243041992 sec - gen_loss = 1.919563030295067, disc_loss = -0.0008525962102950608\n",
      "Time for epoch 16301 is 0.1760110855102539 sec - gen_loss = 1.9151168765524171, disc_loss = -0.0007092514470598087\n",
      "Time for epoch 16401 is 0.18801188468933105 sec - gen_loss = 1.9063497147169848, disc_loss = -0.0006835664435919946\n",
      "Time for epoch 16501 is 0.20001220703125 sec - gen_loss = 1.9111620392117104, disc_loss = -0.0006806261106587696\n",
      "Time for epoch 16601 is 0.22001361846923828 sec - gen_loss = 1.9121056322458598, disc_loss = -0.0007345060186434282\n",
      "Time for epoch 16701 is 0.20801305770874023 sec - gen_loss = 1.9045636875963354, disc_loss = -0.0007061769316358379\n",
      "Time for epoch 16801 is 0.2720167636871338 sec - gen_loss = 1.9103538433516416, disc_loss = -0.0008072837238501284\n",
      "Time for epoch 16901 is 0.2800173759460449 sec - gen_loss = 1.8963478419203088, disc_loss = -0.0006303590287277195\n",
      "Time for epoch 17001 is 0.3440213203430176 sec - gen_loss = 1.914772218243274, disc_loss = -0.000707717674427157\n",
      "Time for epoch 17101 is 0.368023157119751 sec - gen_loss = 1.895701643563241, disc_loss = -0.0007625273379100174\n",
      "Time for epoch 17201 is 0.4720296859741211 sec - gen_loss = 1.8998577401202708, disc_loss = -0.0008694770732445903\n",
      "Time for epoch 17301 is 0.844052791595459 sec - gen_loss = 1.9044073789436715, disc_loss = -0.0007849663204682131\n",
      "Time for epoch 17401 is 1.4000873565673828 sec - gen_loss = 1.899896529204888, disc_loss = -0.0007319352650579376\n",
      "Time for epoch 17501 is 1.3560848236083984 sec - gen_loss = 1.9116026379388276, disc_loss = -0.0009440474901652186\n",
      "Time for epoch 17601 is 1.2760798931121826 sec - gen_loss = 1.9019016023989581, disc_loss = -0.0009022450861913457\n",
      "Time for epoch 17701 is 1.2440779209136963 sec - gen_loss = 1.8956130919321956, disc_loss = -0.0008015104103028947\n",
      "Time for epoch 17801 is 1.1600725650787354 sec - gen_loss = 1.8964386023237576, disc_loss = -0.0006694709449476196\n",
      "Time for epoch 17901 is 1.1480717658996582 sec - gen_loss = 1.8909162308498628, disc_loss = -0.0007525119882906483\n",
      "Time for epoch 18001 is 1.0960683822631836 sec - gen_loss = 1.8971228097477972, disc_loss = -0.0007390404667224716\n",
      "Time for epoch 18101 is 1.044065237045288 sec - gen_loss = 1.882266856628472, disc_loss = -0.0006489612987933432\n",
      "Time for epoch 18201 is 1.0480656623840332 sec - gen_loss = 1.8843505092274664, disc_loss = -0.0007146276925382685\n",
      "Time for epoch 18301 is 1.0040626525878906 sec - gen_loss = 1.8774931888967343, disc_loss = -0.0007612270122629415\n",
      "Time for epoch 18401 is 2.716169834136963 sec - gen_loss = 1.8817816030924353, disc_loss = -0.0008375281788892625\n",
      "Time for epoch 18501 is 3.952247142791748 sec - gen_loss = 1.891048589006947, disc_loss = -0.000609752486097809\n",
      "Time for epoch 18601 is 2.904181480407715 sec - gen_loss = 1.8791403772065252, disc_loss = -0.0008923283990258763\n",
      "Time for epoch 18701 is 2.252140760421753 sec - gen_loss = 1.88447245172892, disc_loss = -0.0005128970380636158\n",
      "Time for epoch 18801 is 2.0401275157928467 sec - gen_loss = 1.8737943186818933, disc_loss = -0.0005025758238960173\n",
      "Time for epoch 18901 is 1.800112247467041 sec - gen_loss = 1.8788875952999533, disc_loss = -0.0005622521350716941\n",
      "Time for epoch 19001 is 1.612100601196289 sec - gen_loss = 1.8675922637879996, disc_loss = -0.0007166785148027645\n",
      "Time for epoch 19101 is 1.5040943622589111 sec - gen_loss = 1.8637174429902585, disc_loss = -0.0006820772035783199\n",
      "Time for epoch 19201 is 1.424088954925537 sec - gen_loss = 1.8544616950993955, disc_loss = -0.0006460168810365829\n",
      "Time for epoch 19301 is 1.2960810661315918 sec - gen_loss = 1.8554416527787463, disc_loss = -0.0007468026875799119\n",
      "Time for epoch 19401 is 1.2680795192718506 sec - gen_loss = 1.855203767357469, disc_loss = -0.0007623352454382499\n",
      "Time for epoch 19501 is 1.216076135635376 sec - gen_loss = 1.853847708938809, disc_loss = -0.0006299764185833006\n",
      "Time for epoch 19601 is 1.124070167541504 sec - gen_loss = 1.8519038109146468, disc_loss = -0.0007676820989618801\n",
      "Time for epoch 19701 is 1.1560721397399902 sec - gen_loss = 1.8500377341606522, disc_loss = -0.0006245452807855381\n",
      "Time for epoch 19801 is 1.132070779800415 sec - gen_loss = 1.8525302163779442, disc_loss = -0.0008546647052755542\n",
      "Time for epoch 19901 is 1.0600662231445312 sec - gen_loss = 1.846607921241658, disc_loss = -0.0008998762636546583\n",
      "Time for epoch 20001 is 1.020063877105713 sec - gen_loss = 1.8461057770093414, disc_loss = -0.0008567648020195561\n",
      "Time for epoch 20101 is 1.0200636386871338 sec - gen_loss = 1.8453585475751622, disc_loss = -0.0006609551686425259\n",
      "Time for epoch 20201 is 3.852240800857544 sec - gen_loss = 1.8379215549314705, disc_loss = -0.000724989603204213\n",
      "Time for epoch 20301 is 4.984311580657959 sec - gen_loss = 1.8485145128855045, disc_loss = -0.0009556110506655158\n",
      "Time for epoch 20401 is 3.208200454711914 sec - gen_loss = 1.8443897261739128, disc_loss = -0.0008517082220784091\n",
      "Time for epoch 20501 is 2.5761609077453613 sec - gen_loss = 1.8359410630090383, disc_loss = -0.0008524985407472272\n",
      "Time for epoch 20601 is 2.1161322593688965 sec - gen_loss = 1.8413422719785544, disc_loss = -0.0006952108524509969\n",
      "Time for epoch 20701 is 1.87611722946167 sec - gen_loss = 1.8418611435210417, disc_loss = -0.0008474130611027019\n",
      "Time for epoch 20801 is 1.684105396270752 sec - gen_loss = 1.8405340878362129, disc_loss = -0.0008274943017508206\n",
      "Time for epoch 20901 is 1.596099853515625 sec - gen_loss = 1.8315725108204184, disc_loss = -0.0009037945134270411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 21001 is 1.4680917263031006 sec - gen_loss = 1.8387538774962906, disc_loss = -0.0009356968191371227\n",
      "Time for epoch 21101 is 1.3920872211456299 sec - gen_loss = 1.835395427386727, disc_loss = -0.0007697100356146774\n",
      "Time for epoch 21201 is 1.3520846366882324 sec - gen_loss = 1.8305165942634984, disc_loss = -0.0006263183945655692\n",
      "Time for epoch 21301 is 1.3000812530517578 sec - gen_loss = 1.8330448462758773, disc_loss = -0.0007999512081327771\n",
      "Time for epoch 21401 is 1.1920745372772217 sec - gen_loss = 1.8321118128454934, disc_loss = -0.0009411209099583855\n",
      "Time for epoch 21501 is 1.1520721912384033 sec - gen_loss = 1.83398213950965, disc_loss = -0.0010325263515491366\n",
      "Time for epoch 21601 is 1.1160697937011719 sec - gen_loss = 1.8343537234739882, disc_loss = -0.0009024315306130768\n",
      "Time for epoch 21701 is 1.0800673961639404 sec - gen_loss = 1.8288091207141117, disc_loss = -0.0008985671694920559\n",
      "Time for epoch 21801 is 1.0800673961639404 sec - gen_loss = 1.8277666547584241, disc_loss = -0.0010404975713296722\n",
      "Time for epoch 21901 is 1.0000624656677246 sec - gen_loss = 1.8288766939272099, disc_loss = -0.0010143741595686228\n",
      "Time for epoch 22001 is 0.9960620403289795 sec - gen_loss = 1.8287725642448143, disc_loss = -0.000915805136719286\n",
      "Time for epoch 22101 is 7.508469343185425 sec - gen_loss = 1.8256068943215014, disc_loss = -0.0007660701580375756\n",
      "Time for epoch 22201 is 4.192261695861816 sec - gen_loss = 1.826105029101404, disc_loss = -0.000972245169761855\n",
      "Time for epoch 22301 is 3.0801925659179688 sec - gen_loss = 1.8223564598162638, disc_loss = -0.0009096354905026872\n",
      "Time for epoch 22401 is 2.4081506729125977 sec - gen_loss = 1.8266360396166055, disc_loss = -0.0009403265503537355\n",
      "Time for epoch 22501 is 2.076129674911499 sec - gen_loss = 1.8200200803795537, disc_loss = -0.0009065276610082881\n",
      "Time for epoch 22601 is 1.8201138973236084 sec - gen_loss = 1.820446352625713, disc_loss = -0.0010906995474998613\n",
      "Time for epoch 22701 is 1.6601040363311768 sec - gen_loss = 1.8210477272981183, disc_loss = -0.0010033629349233335\n",
      "Time for epoch 22801 is 1.5240952968597412 sec - gen_loss = 1.8196682499678507, disc_loss = -0.0010043040698827557\n",
      "Time for epoch 22901 is 1.424088716506958 sec - gen_loss = 1.8147473477318494, disc_loss = -0.0008674856382505003\n",
      "Time for epoch 23001 is 1.3800861835479736 sec - gen_loss = 1.8208003097341536, disc_loss = -0.001021510482621834\n",
      "Time for epoch 23101 is 1.320082426071167 sec - gen_loss = 1.8237794200025168, disc_loss = -0.0008250037753693486\n",
      "Time for epoch 23201 is 1.2680792808532715 sec - gen_loss = 1.814888138181888, disc_loss = -0.0008651080629777932\n",
      "Time for epoch 23301 is 1.3000812530517578 sec - gen_loss = 1.8114298278662528, disc_loss = -0.0010538903325200424\n",
      "Time for epoch 23401 is 1.2600789070129395 sec - gen_loss = 1.8162514818506696, disc_loss = -0.0010501565535013678\n",
      "Time for epoch 23501 is 1.2040753364562988 sec - gen_loss = 1.816834081479529, disc_loss = -0.0010188206144375182\n",
      "Time for epoch 23601 is 1.1160697937011719 sec - gen_loss = 1.8172965661401193, disc_loss = -0.0008458887023658485\n",
      "Time for epoch 23701 is 1.1360712051391602 sec - gen_loss = 1.816956274581916, disc_loss = -0.0009193488146061999\n",
      "Time for epoch 23801 is 1.1080691814422607 sec - gen_loss = 1.8217110465844797, disc_loss = -0.0009117592729356533\n",
      "Time for epoch 23901 is 1.0720670223236084 sec - gen_loss = 1.8143599530916856, disc_loss = -0.0009291208298755109\n",
      "Time for epoch 24001 is 1.0520658493041992 sec - gen_loss = 1.813193589924844, disc_loss = -0.0007113086989651446\n",
      "Time for epoch 24101 is 1.0120632648468018 sec - gen_loss = 1.8098051068570211, disc_loss = -0.0009189049946310868\n",
      "Time for epoch 24201 is 1.0000624656677246 sec - gen_loss = 1.813722896821406, disc_loss = -0.0008732502124295108\n",
      "Time for epoch 24301 is 0.9840617179870605 sec - gen_loss = 1.8140102677305114, disc_loss = -0.0009201043774484852\n",
      "Time for epoch 24401 is 5.7843616008758545 sec - gen_loss = 1.810990291035173, disc_loss = -0.0010841200303863452\n",
      "Time for epoch 24501 is 6.084380626678467 sec - gen_loss = 1.8101811921555988, disc_loss = -0.0008719992424694635\n",
      "Time for epoch 24601 is 3.6202261447906494 sec - gen_loss = 1.8068027566268619, disc_loss = -0.0008473904137148441\n",
      "Time for epoch 24701 is 2.8401777744293213 sec - gen_loss = 1.8117621814485085, disc_loss = -0.0007742789147584159\n",
      "Time for epoch 24801 is 2.524157762527466 sec - gen_loss = 1.8071550388320299, disc_loss = -0.0008877752078301576\n",
      "Time for epoch 24901 is -8.335013628005981 sec - gen_loss = 1.8082025950574332, disc_loss = -0.0009295743369640416\n",
      "Time for epoch 25001 is 2.0241265296936035 sec - gen_loss = 1.8085757666310764, disc_loss = -0.0009625374808345281\n",
      "Time for epoch 25101 is 1.9201200008392334 sec - gen_loss = 1.8054679931013011, disc_loss = -0.0009094173601187982\n",
      "Time for epoch 25201 is 1.8201136589050293 sec - gen_loss = 1.8003207953983829, disc_loss = -0.0008160905645651478\n",
      "Time for epoch 25301 is 1.6881053447723389 sec - gen_loss = 1.8042803300535348, disc_loss = -0.0008921232288722495\n",
      "Time for epoch 25401 is -10.622449398040771 sec - gen_loss = 1.8037165404671351, disc_loss = -0.0008394701018413833\n",
      "Time for epoch 25501 is 1.5480968952178955 sec - gen_loss = 1.8020587747239587, disc_loss = -0.00087860224221785\n",
      "Time for epoch 25601 is 1.4400899410247803 sec - gen_loss = 1.7910409919247954, disc_loss = -0.000835159337787621\n",
      "Time for epoch 25701 is 1.4640915393829346 sec - gen_loss = 1.7850233166758986, disc_loss = -0.0007275434722886449\n",
      "Time for epoch 25801 is -13.787076234817505 sec - gen_loss = 1.803138888989872, disc_loss = -0.0008242519506091207\n",
      "Time for epoch 25901 is 1.3760859966278076 sec - gen_loss = 1.8017205444051836, disc_loss = -0.0009712938640864005\n",
      "Time for epoch 26001 is 1.2680792808532715 sec - gen_loss = 1.7997811821087373, disc_loss = -0.0005932496894171317\n",
      "Time for epoch 26101 is 1.220076322555542 sec - gen_loss = 1.7978633254299086, disc_loss = -0.0008672237802748673\n",
      "Time for epoch 26201 is 1.1680731773376465 sec - gen_loss = 1.7974117196596462, disc_loss = -0.0010655526931794574\n",
      "Time for epoch 26301 is 1.1640725135803223 sec - gen_loss = 1.7939610898802367, disc_loss = -0.0008609436866786176\n",
      "Time for epoch 26401 is 1.1640725135803223 sec - gen_loss = 1.7956827856576605, disc_loss = -0.000988729077063278\n",
      "Time for epoch 26501 is 1.1560723781585693 sec - gen_loss = 1.7891063420911302, disc_loss = -0.0006834205119586589\n",
      "Time for epoch 26601 is 1.1120696067810059 sec - gen_loss = 1.791265029161836, disc_loss = -0.0008639544140470306\n",
      "Time for epoch 26701 is 1.0680668354034424 sec - gen_loss = 1.7913686801465554, disc_loss = -0.0006964407352327164\n",
      "Time for epoch 26801 is 1.040065050125122 sec - gen_loss = 1.7898280472536834, disc_loss = -0.0009032512266948644\n",
      "Time for epoch 26901 is 1.048065423965454 sec - gen_loss = 1.7895134716738386, disc_loss = -0.0008804169655361301\n",
      "Time for epoch 27001 is 1.036064863204956 sec - gen_loss = 1.7930855466006745, disc_loss = -0.0012234229187490977\n",
      "Time for epoch 27101 is 4.464278697967529 sec - gen_loss = 1.7886373464982954, disc_loss = -0.0009275969758428211\n",
      "Time for epoch 27201 is 6.368397951126099 sec - gen_loss = 1.786784284975715, disc_loss = -0.0007176017180733228\n",
      "Time for epoch 27301 is 3.8282392024993896 sec - gen_loss = 1.79280521722746, disc_loss = -0.0008805024160172118\n",
      "Time for epoch 27401 is 3.120194911956787 sec - gen_loss = 1.7911713022145526, disc_loss = -0.0008874805431194907\n",
      "Time for epoch 27501 is 2.556159734725952 sec - gen_loss = 1.7921124849177057, disc_loss = -0.0008815452519350453\n",
      "Time for epoch 27601 is -8.587864398956299 sec - gen_loss = 1.7859265548406225, disc_loss = -0.0008830181974210144\n",
      "Time for epoch 27701 is 2.0241265296936035 sec - gen_loss = 1.7845241118915423, disc_loss = -0.0008499298897192423\n",
      "Time for epoch 27801 is 1.9121196269989014 sec - gen_loss = 1.7858934040068586, disc_loss = -0.0008901836105580738\n",
      "Time for epoch 27901 is 1.7721104621887207 sec - gen_loss = 1.7840165129992174, disc_loss = -0.0009778735756918859\n",
      "Time for epoch 28001 is 1.7121071815490723 sec - gen_loss = 1.7902847455096118, disc_loss = -0.0007528690763870188\n",
      "Time for epoch 28101 is 1.600099802017212 sec - gen_loss = 1.787637120353549, disc_loss = -0.0006884151303570552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 28201 is 1.512094497680664 sec - gen_loss = 1.786298824455905, disc_loss = -0.0006990052837200242\n",
      "Time for epoch 28301 is 1.4600913524627686 sec - gen_loss = 1.7816512699154503, disc_loss = -0.0008874293041725443\n",
      "Time for epoch 28401 is 1.3720858097076416 sec - gen_loss = 1.779432303299382, disc_loss = -0.0009803911125047323\n",
      "Time for epoch 28501 is 1.3640854358673096 sec - gen_loss = 1.7826725789678375, disc_loss = -0.0008644312748909795\n",
      "Time for epoch 28601 is 1.2760796546936035 sec - gen_loss = 1.7838625844101488, disc_loss = -0.0009427820845376723\n",
      "Time for epoch 28701 is 1.2520780563354492 sec - gen_loss = 1.7797105642093594, disc_loss = -0.0008338589572194736\n",
      "Time for epoch 28801 is 1.23207688331604 sec - gen_loss = 1.7841314075061787, disc_loss = -0.0008220174976492232\n",
      "Time for epoch 28901 is 1.2080750465393066 sec - gen_loss = 1.780188798165522, disc_loss = -0.001091153841483681\n",
      "Time for epoch 29001 is 1.1000688076019287 sec - gen_loss = 1.7802482113623586, disc_loss = -0.0008811694087603143\n",
      "Time for epoch 29101 is 1.0640666484832764 sec - gen_loss = 1.7860962922733326, disc_loss = -0.0011289556381306638\n",
      "Time for epoch 29201 is 1.0760672092437744 sec - gen_loss = 1.7806367824465572, disc_loss = -0.0008954973746214007\n",
      "Time for epoch 29301 is 1.0600662231445312 sec - gen_loss = 1.777845004130894, disc_loss = -0.0007502819091208458\n",
      "Time for epoch 29401 is 1.036064863204956 sec - gen_loss = 1.7779998347689603, disc_loss = -0.0008513937471885342\n",
      "Time for epoch 29501 is 0.9920618534088135 sec - gen_loss = 1.7800915376684872, disc_loss = -0.0009619204736983378\n",
      "Time for epoch 29601 is 0.9600598812103271 sec - gen_loss = 1.7771964014814434, disc_loss = -0.000820066697610575\n",
      "Time for epoch 29701 is 6.800425052642822 sec - gen_loss = 1.7741541038562785, disc_loss = -0.0008546831730184903\n",
      "Time for epoch 29801 is 5.29633092880249 sec - gen_loss = 1.7785319152439862, disc_loss = -0.0009727222140837743\n",
      "Time for epoch 29901 is 3.472217082977295 sec - gen_loss = 1.7804500311072622, disc_loss = -0.00113927612368129\n",
      "Time for epoch 30001 is 3.012188196182251 sec - gen_loss = 1.7785434801491427, disc_loss = -0.001072633465253722\n",
      "Time for epoch 30101 is 2.5681605339050293 sec - gen_loss = 1.7740724295670374, disc_loss = -0.0008562771550382783\n",
      "Time for epoch 30201 is 2.460153818130493 sec - gen_loss = 1.7719145616759886, disc_loss = -0.0006651094958486526\n",
      "Time for epoch 30301 is 2.1281328201293945 sec - gen_loss = 1.781259812103144, disc_loss = -0.0008992028948118659\n",
      "Time for epoch 30401 is 1.884117841720581 sec - gen_loss = 1.770394173154735, disc_loss = -0.0010274753126586426\n",
      "Time for epoch 30501 is 1.7721104621887207 sec - gen_loss = 1.775727407660892, disc_loss = -0.0010142976281401629\n",
      "Time for epoch 30601 is 1.692105770111084 sec - gen_loss = 1.7717689322527077, disc_loss = -0.0009546132766590487\n",
      "Time for epoch 30701 is 1.5560970306396484 sec - gen_loss = 1.7674579853905836, disc_loss = -0.0008302842594832075\n",
      "Time for epoch 30801 is 1.4560911655426025 sec - gen_loss = 1.7712496944211258, disc_loss = -0.0007589446033650074\n",
      "Time for epoch 30901 is 1.3920869827270508 sec - gen_loss = 1.7778578641044966, disc_loss = -0.0007854620224343504\n",
      "Time for epoch 31001 is 1.3360836505889893 sec - gen_loss = 1.7720564963231038, disc_loss = -0.0008957786723196596\n",
      "Time for epoch 31101 is 1.3000812530517578 sec - gen_loss = 1.7740087852301307, disc_loss = -0.0009019740498574525\n",
      "Time for epoch 31201 is 1.2560784816741943 sec - gen_loss = 1.7734597637077354, disc_loss = -0.000890839471564789\n",
      "Time for epoch 31301 is 1.2040750980377197 sec - gen_loss = 1.7717300120206003, disc_loss = -0.0009119779524754691\n",
      "Time for epoch 31401 is 1.1760737895965576 sec - gen_loss = 1.770490949066369, disc_loss = -0.000942574464253398\n",
      "Time for epoch 31501 is 1.136070966720581 sec - gen_loss = 1.7710980650406503, disc_loss = -0.0010889784116039428\n",
      "Time for epoch 31601 is 1.1000685691833496 sec - gen_loss = 1.7728910550015227, disc_loss = -0.00102837505028688\n",
      "Time for epoch 31701 is 1.1000688076019287 sec - gen_loss = 1.772077933387929, disc_loss = -0.0006586237417955992\n",
      "Time for epoch 31801 is -19.936567306518555 sec - gen_loss = 1.76946522551503, disc_loss = -0.0010078466226875243\n",
      "Time for epoch 31901 is 1.0600662231445312 sec - gen_loss = 1.7669897655792912, disc_loss = -0.0006915572555963136\n",
      "Time for epoch 32001 is 1.0600662231445312 sec - gen_loss = 1.7672706589123928, disc_loss = -0.0007543370281975359\n",
      "Time for epoch 32101 is 1.032064437866211 sec - gen_loss = 1.7713726679616746, disc_loss = -0.0008127807419980855\n",
      "Time for epoch 32201 is 1.0080628395080566 sec - gen_loss = 1.7709726173960147, disc_loss = -0.001005997400298925\n",
      "Time for epoch 32301 is -22.38359260559082 sec - gen_loss = 1.768546671913528, disc_loss = -0.0010621741935274774\n",
      "Time for epoch 32401 is 7.5684731006622314 sec - gen_loss = 1.7680338629219199, disc_loss = -0.0011710125969373003\n",
      "Time for epoch 32501 is 4.512281894683838 sec - gen_loss = 1.762637347063477, disc_loss = -0.0008524858976135027\n",
      "Time for epoch 32601 is 3.0401899814605713 sec - gen_loss = 1.7687942550527276, disc_loss = -0.0009016777750799737\n",
      "Time for epoch 32701 is 2.524157762527466 sec - gen_loss = 1.7722865052314811, disc_loss = -0.0007864351546460292\n",
      "Time for epoch 32801 is 2.336146116256714 sec - gen_loss = 1.7657224653170311, disc_loss = -0.0010041023522608455\n",
      "Time for epoch 32901 is 2.2121381759643555 sec - gen_loss = 1.762901888735266, disc_loss = -0.0010628576109658\n",
      "Time for epoch 33001 is 1.984123706817627 sec - gen_loss = 1.763586600333348, disc_loss = -0.0008547992037635456\n",
      "Time for epoch 33101 is 1.87611722946167 sec - gen_loss = 1.7623854502422511, disc_loss = -0.0010030123505999438\n",
      "Time for epoch 33201 is 1.7521095275878906 sec - gen_loss = 1.7634083627661588, disc_loss = -0.0009160673546357865\n",
      "Time for epoch 33301 is 1.6241014003753662 sec - gen_loss = 1.767381427634722, disc_loss = -0.0008233718252178317\n",
      "Time for epoch 33401 is 1.512094497680664 sec - gen_loss = 1.764291237837666, disc_loss = -0.0010062290984597906\n",
      "Time for epoch 33501 is 1.4800927639007568 sec - gen_loss = 1.7654169437376133, disc_loss = -0.0008353216358973463\n",
      "Time for epoch 33601 is 1.404087781906128 sec - gen_loss = 1.7608399585225203, disc_loss = -0.0009040552771165993\n",
      "Time for epoch 33701 is 1.3920869827270508 sec - gen_loss = 1.7584795659170045, disc_loss = -0.0008958613693948455\n",
      "Time for epoch 33801 is 1.2840805053710938 sec - gen_loss = 1.7576423100126441, disc_loss = -0.0009905395438950819\n",
      "Time for epoch 33901 is 1.2440779209136963 sec - gen_loss = 1.7574700261461933, disc_loss = -0.0008308006060556434\n",
      "Time for epoch 34001 is 1.2160758972167969 sec - gen_loss = 1.7655668116635175, disc_loss = -0.0007911871210093777\n",
      "Time for epoch 34101 is 1.1560723781585693 sec - gen_loss = 1.7632409210910718, disc_loss = -0.0010187877930455298\n",
      "Time for epoch 34201 is 1.1640727519989014 sec - gen_loss = 1.760292386685776, disc_loss = -0.0009451917600277023\n",
      "Time for epoch 34301 is 1.12807035446167 sec - gen_loss = 1.7618227920972285, disc_loss = -0.0009293716012065996\n",
      "Time for epoch 34401 is 1.1160695552825928 sec - gen_loss = 1.7663629314998461, disc_loss = -0.0008856115723910764\n",
      "Time for epoch 34501 is 1.0600662231445312 sec - gen_loss = 1.7572548266976828, disc_loss = -0.0008447282177023431\n",
      "Time for epoch 34601 is 1.0640666484832764 sec - gen_loss = 1.7605056325267459, disc_loss = -0.0006941795286110132\n",
      "Time for epoch 34701 is 1.020063877105713 sec - gen_loss = 1.7586919208921568, disc_loss = -0.0009719526174447729\n",
      "Time for epoch 34801 is 0.9920618534088135 sec - gen_loss = 1.751708317373637, disc_loss = -0.0010491416035017494\n",
      "Time for epoch 34901 is 1.0160634517669678 sec - gen_loss = 1.7698067792944472, disc_loss = -0.0010018256196426072\n",
      "Time for epoch 35001 is 1.0040626525878906 sec - gen_loss = 1.761753582538627, disc_loss = -0.001107696234771579\n",
      "Time for epoch 35101 is 6.3483967781066895 sec - gen_loss = 1.756677146880799, disc_loss = -0.0010134098193256196\n",
      "Time for epoch 35201 is 3.564222812652588 sec - gen_loss = 1.755654992648139, disc_loss = -0.0010355698399772525\n",
      "Time for epoch 35301 is 3.0601911544799805 sec - gen_loss = 1.7593447881140873, disc_loss = -0.0006955524629396125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 35401 is -8.770188570022583 sec - gen_loss = 1.7588294513247777, disc_loss = -0.0011794783772030564\n",
      "Time for epoch 35501 is 2.2241389751434326 sec - gen_loss = 1.7549700698447788, disc_loss = -0.0007872440791965067\n",
      "Time for epoch 35601 is 2.068129062652588 sec - gen_loss = 1.7588619090680337, disc_loss = -0.0007908146849554193\n",
      "Time for epoch 35701 is 1.8441152572631836 sec - gen_loss = 1.7613282439767308, disc_loss = -0.0007038530493938754\n",
      "Time for epoch 35801 is 1.7761108875274658 sec - gen_loss = 1.757644531883427, disc_loss = -0.0010245834148514938\n",
      "Time for epoch 35901 is 1.6641039848327637 sec - gen_loss = 1.7580801659305847, disc_loss = -0.001074121350949472\n",
      "Time for epoch 36001 is 1.5760984420776367 sec - gen_loss = 1.7585080196695313, disc_loss = -0.0010210133938568242\n",
      "Time for epoch 36101 is 1.4880928993225098 sec - gen_loss = 1.7564098295199833, disc_loss = -0.0008719104846814299\n",
      "Time for epoch 36201 is 1.4320895671844482 sec - gen_loss = 1.7561026165760747, disc_loss = -0.0009356218853119474\n",
      "Time for epoch 36301 is 1.3800861835479736 sec - gen_loss = 1.7522405940763934, disc_loss = -0.0007426231405557209\n",
      "Time for epoch 36401 is 1.3120818138122559 sec - gen_loss = 1.7606951779761724, disc_loss = -0.0008449668881718244\n",
      "Time for epoch 36501 is 1.2560784816741943 sec - gen_loss = 1.7608161100800082, disc_loss = -0.001018340767031849\n",
      "Time for epoch 36601 is 1.21207594871521 sec - gen_loss = 1.7555031123872313, disc_loss = -0.0011162153825081005\n",
      "Time for epoch 36701 is 1.1800737380981445 sec - gen_loss = 1.7574639767548215, disc_loss = -0.00120845939216465\n",
      "Time for epoch 36801 is 1.1560721397399902 sec - gen_loss = 1.7570929719420825, disc_loss = -0.0009157490366515544\n",
      "Time for epoch 36901 is 1.124070167541504 sec - gen_loss = 1.7431501528925955, disc_loss = -0.0007767542787059082\n",
      "Time for epoch 37001 is 1.120069980621338 sec - gen_loss = 1.7537230511733166, disc_loss = -0.0003867883237112761\n",
      "Time for epoch 37101 is 1.0560660362243652 sec - gen_loss = 1.752946846743972, disc_loss = -0.0008037327357904361\n",
      "Time for epoch 37201 is 1.0640666484832764 sec - gen_loss = 1.7578546150865313, disc_loss = -0.0009442885318549493\n",
      "Time for epoch 37301 is 1.048065423965454 sec - gen_loss = 1.75723400624992, disc_loss = -0.001191724122609325\n",
      "Time for epoch 37401 is 1.0120632648468018 sec - gen_loss = 1.7564673015351122, disc_loss = -0.0007745033253304656\n",
      "Time for epoch 37501 is 0.9720606803894043 sec - gen_loss = 1.7557773649007336, disc_loss = -0.0007433914604378972\n",
      "Time for epoch 37601 is 3.4562158584594727 sec - gen_loss = 1.754348502647622, disc_loss = -0.000994975205910719\n",
      "Time for epoch 37701 is 7.408462762832642 sec - gen_loss = 1.7561412116159083, disc_loss = -0.0008673388041542165\n",
      "Time for epoch 37801 is -12.72078013420105 sec - gen_loss = 1.7522946146399025, disc_loss = -0.0008050127652427857\n",
      "Time for epoch 37901 is 3.3162074089050293 sec - gen_loss = 1.7480737920302536, disc_loss = -0.0010762616002077435\n",
      "Time for epoch 38001 is 2.728170394897461 sec - gen_loss = 1.7444719029570153, disc_loss = -0.0008756335968833302\n",
      "Time for epoch 38101 is 2.4921555519104004 sec - gen_loss = 1.7559418484317784, disc_loss = -0.0008753852153642761\n",
      "Time for epoch 38201 is -7.913542747497559 sec - gen_loss = 1.7494117285847945, disc_loss = -0.0009632557631600777\n",
      "Time for epoch 38301 is 1.9321205615997314 sec - gen_loss = 1.7553208005464762, disc_loss = -0.0010406130428989454\n",
      "Time for epoch 38401 is 1.8001124858856201 sec - gen_loss = 1.7520206030554855, disc_loss = -0.0009478876031539801\n",
      "Time for epoch 38501 is 1.700106143951416 sec - gen_loss = 1.748279859937526, disc_loss = -0.0010034342843611896\n",
      "Time for epoch 38601 is 1.6121008396148682 sec - gen_loss = 1.7482797700822263, disc_loss = -0.0009502200979835687\n",
      "Time for epoch 38701 is -11.112080812454224 sec - gen_loss = 1.7509480059066644, disc_loss = -0.0008795293867433619\n",
      "Time for epoch 38801 is 1.504093885421753 sec - gen_loss = 1.7488724388688492, disc_loss = -0.0010526521624390503\n",
      "Time for epoch 38901 is 1.4520907402038574 sec - gen_loss = 1.750490666289023, disc_loss = -0.000956407543264148\n",
      "Time for epoch 39001 is 1.3640851974487305 sec - gen_loss = 1.7461663058962738, disc_loss = -0.0009046685779377572\n",
      "Time for epoch 39101 is 1.316082239151001 sec - gen_loss = 1.7509922114604013, disc_loss = -0.0010311906866936774\n",
      "Time for epoch 39201 is 1.2640788555145264 sec - gen_loss = 1.7498635186643456, disc_loss = -0.0011719051672820372\n",
      "Time for epoch 39301 is 1.2120757102966309 sec - gen_loss = 1.7512570366884823, disc_loss = -0.0009399939409117746\n",
      "Time for epoch 39401 is 1.1800737380981445 sec - gen_loss = 1.746185702278948, disc_loss = -0.0011804882108120969\n",
      "Time for epoch 39501 is 1.1760735511779785 sec - gen_loss = 1.7504756838630915, disc_loss = -0.0010310019861996752\n",
      "Time for epoch 39601 is 1.1840741634368896 sec - gen_loss = 1.7525382646874137, disc_loss = -0.0009743004425426903\n",
      "Time for epoch 39701 is 1.132070541381836 sec - gen_loss = 1.7484415127210415, disc_loss = -0.0010328947350457295\n",
      "Time for epoch 39801 is 1.120070219039917 sec - gen_loss = 1.7511149524003384, disc_loss = -0.001201813681903963\n",
      "Time for epoch 39901 is 1.0800676345825195 sec - gen_loss = 1.7537322963774769, disc_loss = -0.00103220783738955\n",
      "Time for epoch 40001 is 1.036064624786377 sec - gen_loss = 1.7534435071558494, disc_loss = -0.0009122821977927351\n",
      "Time for epoch 40101 is 1.0520658493041992 sec - gen_loss = 1.7490099577541574, disc_loss = -0.0009554772425116915\n",
      "Time for epoch 40201 is 1.024064064025879 sec - gen_loss = 1.7493962010605197, disc_loss = -0.0008877974430342225\n",
      "Time for epoch 40301 is 0.9880616664886475 sec - gen_loss = 1.7484951917598717, disc_loss = -0.0010227032976818215\n",
      "Time for epoch 40401 is 4.896305799484253 sec - gen_loss = 1.744013009343913, disc_loss = -0.0011345219874105083\n",
      "Time for epoch 40501 is 6.020376205444336 sec - gen_loss = 1.7489485956891455, disc_loss = -0.0009692482296690536\n",
      "Time for epoch 40601 is 3.968248128890991 sec - gen_loss = 1.7458722335104413, disc_loss = -0.001143880624593314\n",
      "Time for epoch 40701 is 2.8761796951293945 sec - gen_loss = 1.742145615223458, disc_loss = -0.0012342432221776987\n",
      "Time for epoch 40801 is 2.540158748626709 sec - gen_loss = 1.7412415417661062, disc_loss = -0.0010117415040474004\n",
      "Time for epoch 40901 is 2.1961371898651123 sec - gen_loss = 1.7435000567049492, disc_loss = -0.0011176713218642325\n",
      "Time for epoch 41001 is 2.1121323108673096 sec - gen_loss = 1.7445851335477187, disc_loss = -0.000959515798660367\n",
      "Time for epoch 41101 is 1.9121193885803223 sec - gen_loss = 1.742568492711366, disc_loss = -0.0009483297236024168\n",
      "Time for epoch 41201 is 1.8121132850646973 sec - gen_loss = 1.7429729315439704, disc_loss = -0.0010242327968989558\n",
      "Time for epoch 41301 is 1.7401087284088135 sec - gen_loss = 1.7418488122909226, disc_loss = -0.000790491980360717\n",
      "Time for epoch 41401 is 1.616100788116455 sec - gen_loss = 1.741963160900847, disc_loss = -0.0008951287081307169\n",
      "Time for epoch 41501 is 1.5560972690582275 sec - gen_loss = 1.7378466563738244, disc_loss = -0.0010579445432809431\n",
      "Time for epoch 41601 is 1.4560909271240234 sec - gen_loss = 1.726725947622186, disc_loss = -0.0008695642835776226\n",
      "Time for epoch 41701 is 1.3480842113494873 sec - gen_loss = 1.746174413016905, disc_loss = -0.000847236131299282\n",
      "Time for epoch 41801 is 1.31608247756958 sec - gen_loss = 1.745077568546779, disc_loss = -0.0010855725481134443\n",
      "Time for epoch 41901 is 1.3440840244293213 sec - gen_loss = 1.7411282548266, disc_loss = -0.0010904946223358226\n",
      "Time for epoch 42001 is 1.220076322555542 sec - gen_loss = 1.746281901942692, disc_loss = -0.0010913780953468967\n",
      "Time for epoch 42101 is 1.1960747241973877 sec - gen_loss = 1.7456367995431208, disc_loss = -0.0010717587785485769\n",
      "Time for epoch 42201 is 0.14113593101501465 sec - gen_loss = 1.7466466753786392, disc_loss = -0.0008028670978218165\n",
      "Time for epoch 42301 is 0.043608903884887695 sec - gen_loss = 1.7420568578901272, disc_loss = -0.000930747539817068\n",
      "Time for epoch 42401 is 0.039792537689208984 sec - gen_loss = 1.744733902464209, disc_loss = -0.0008439046014381642\n",
      "Time for epoch 42501 is 0.035875797271728516 sec - gen_loss = 1.7466973315447267, disc_loss = -0.000942237630950539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 42601 is 0.027944326400756836 sec - gen_loss = 1.7430660785672716, disc_loss = -0.0007946011087340407\n",
      "Time for epoch 42701 is 0.02796316146850586 sec - gen_loss = 1.742919891920419, disc_loss = -0.0010394282137846716\n",
      "Time for epoch 42801 is 0.035968780517578125 sec - gen_loss = 1.7422995550300864, disc_loss = -0.0010749845529662522\n",
      "Time for epoch 42901 is 0.0279843807220459 sec - gen_loss = 1.743029534616671, disc_loss = -0.0010627764697131547\n",
      "Time for epoch 43001 is 0.03198862075805664 sec - gen_loss = 1.7412586153336398, disc_loss = -0.0010999503387612646\n",
      "Time for epoch 43101 is 0.028001785278320312 sec - gen_loss = 1.745854023293393, disc_loss = -0.0009949086503620052\n",
      "Time for epoch 43201 is 0.03200197219848633 sec - gen_loss = 1.7429394832553895, disc_loss = -0.0010020432507296712\n",
      "Time for epoch 43301 is 0.028001785278320312 sec - gen_loss = 1.748574556864839, disc_loss = -0.0009884878494231183\n",
      "Time for epoch 43401 is 0.036002397537231445 sec - gen_loss = 1.7452412889570814, disc_loss = -0.0009352637575460261\n",
      "Time for epoch 43501 is 0.028001785278320312 sec - gen_loss = 1.7331617555716194, disc_loss = -0.0009216511071263008\n",
      "Time for epoch 43601 is 0.02800154685974121 sec - gen_loss = 1.7440340039357238, disc_loss = -0.0008529366217166045\n",
      "Time for epoch 43701 is 0.03200221061706543 sec - gen_loss = 1.7431887420640975, disc_loss = -0.0009836469094171908\n",
      "Time for epoch 43801 is 0.028001785278320312 sec - gen_loss = 1.7436047028718904, disc_loss = -0.0009910555714073466\n",
      "Time for epoch 43901 is 0.02800154685974121 sec - gen_loss = 1.7399631162208202, disc_loss = -0.0008881154766692131\n",
      "Time for epoch 44001 is 0.04000234603881836 sec - gen_loss = 1.7402632897010946, disc_loss = -0.0010320600359850875\n",
      "Time for epoch 44101 is 0.028001785278320312 sec - gen_loss = 1.740372914572818, disc_loss = -0.0007223425789833736\n",
      "Time for epoch 44201 is 0.036002397537231445 sec - gen_loss = 1.7370599273474188, disc_loss = -0.0011785962214880866\n",
      "Time for epoch 44301 is 0.03200197219848633 sec - gen_loss = 1.7391780713869058, disc_loss = -0.001132273292101033\n",
      "Time for epoch 44401 is 0.03200197219848633 sec - gen_loss = 1.7384293483573068, disc_loss = -0.0008459038922004737\n",
      "Time for epoch 44501 is 0.028001785278320312 sec - gen_loss = 1.742332450219013, disc_loss = -0.0009678224171867778\n",
      "Time for epoch 44601 is 0.028001785278320312 sec - gen_loss = 1.7391147815966017, disc_loss = -0.0011541127886814473\n",
      "Time for epoch 44701 is 0.03200173377990723 sec - gen_loss = 1.7476618562545534, disc_loss = -0.0009830377798233799\n",
      "Time for epoch 44801 is 0.028001785278320312 sec - gen_loss = 1.7387636903298336, disc_loss = -0.001185478467601165\n",
      "Time for epoch 44901 is 0.036002397537231445 sec - gen_loss = 1.7404243908222232, disc_loss = -0.0010684634231083413\n"
     ]
    }
   ],
   "source": [
    "# separate to 0,1 dataset\n",
    "data_1=X_train.loc[X_train['elapsed_class']==1]\n",
    "data_0=X_train.loc[X_train['elapsed_class']==0]\n",
    "## store losses\n",
    "### generator losses\n",
    "losses_gen = np.array([])\n",
    "best_loss_gen = np.inf\n",
    "### discriminator losses\n",
    "losses_dis = np.array([])\n",
    "best_loss_dis = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    disc_loss = 0\n",
    "    gen_loss = 0\n",
    "\n",
    "    # resample the dataset\n",
    "    data1_shape_0 = data_1.sample(data_0.shape[0])\n",
    "    df_same_shape = pd.concat([data1_shape_0, data_0]).to_numpy()\n",
    "\n",
    "#     # slices to data and labels\n",
    "#     df_same_ = df_same_shape.iloc[:, :-1].to_numpy()\n",
    "#     org_label = df_same_shape.iloc[:, -1].to_numpy().reshape(-1, 1)\n",
    "\n",
    "    # create batch dataset\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices(df_same_shape)\\\n",
    "        .shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    for data in training_dataset:\n",
    "        for _ in range(n_critic): # 5*discriminator times, 1*generator times\n",
    "            disc_loss += train_discriminator(data, generator,\n",
    "                                             discriminator, disc_opt, latent_dim)\n",
    "#         if disc_opt.iterations.numpy() % n_critic == 0: ### using samples\n",
    "        gen_loss+= train_generator(data, generator,\n",
    "                                    discriminator, gen_opt, params, batch_size, latent_dim)\n",
    "    \n",
    "    losses_gen= np.append(losses_gen, gen_loss / batch_size)\n",
    "    losses_dis= np.append(losses_dis, disc_loss / (batch_size*n_critic))\n",
    "    \n",
    "    ## every 100 epochs print gen_loss and dis_loss\n",
    "    if (epoch+1)%100==0 or epochs==1:\n",
    "        print('Time for epoch {} is {} sec - gen_loss = {}, disc_loss = {}'.format(epoch + 1, time.time() - start,\n",
    "                                                                               gen_loss / batch_size,\n",
    "                                                                               disc_loss / (batch_size*n_critic)))\n",
    "    # save best discriminator or generator\n",
    "    if abs(best_loss_gen) > abs((gen_loss / batch_size)):\n",
    "        best_loss_gen = (gen_loss / batch_size)\n",
    "        generator.save_weights(checkpoint_prefix+\"gen\", save_format='tf')\n",
    "        \n",
    "    if abs(best_loss_dis) > abs((disc_loss / (batch_size*n_critic))):\n",
    "        best_loss_dis = (disc_loss / (batch_size*n_critic))\n",
    "        discriminator.save_weights(checkpoint_prefix+\"dis\", save_format='tf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Record the 40000 gen_loss = , disc_loss = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"ADS-GAN-constraint training Loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "# plt.ylim(-3,50)\n",
    "plt.grid()\n",
    "plt.plot(losses_gen, label='Generator')\n",
    "plt.plot(losses_dis, label='Discriminator')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(\"./ADS-GAN-constraint_LOSS.png\",dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(896, 63)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create matrix 0 row*latent_dim columns\n",
    "arr=np.empty((0,latent_dim))\n",
    "noise = tf.random.normal([128, latent_dim])\n",
    "\n",
    "## batch testing data\n",
    "testing_dataset = tf.data.Dataset.from_tensor_slices(X_test.to_numpy())\\\n",
    "        .shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
    "\n",
    "## generate dataset\n",
    "for data in testing_dataset:\n",
    "    gen_=generator(noise, data).numpy()\n",
    "    arr=np.append(arr,gen_,axis=0)\n",
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BT_NM</th>\n",
       "      <th>HR_NM</th>\n",
       "      <th>RR_NM</th>\n",
       "      <th>HB_NM</th>\n",
       "      <th>HCT_NM</th>\n",
       "      <th>PLATELET_NM</th>\n",
       "      <th>WBC_NM</th>\n",
       "      <th>PTT1_NM</th>\n",
       "      <th>PTT2_NM</th>\n",
       "      <th>PTINR_NM</th>\n",
       "      <th>ER_NM</th>\n",
       "      <th>BUN_NM</th>\n",
       "      <th>CRE_NM</th>\n",
       "      <th>BMI</th>\n",
       "      <th>age</th>\n",
       "      <th>PPD</th>\n",
       "      <th>THDA_FL</th>\n",
       "      <th>THDH_FL</th>\n",
       "      <th>THDI_FL</th>\n",
       "      <th>THDAM_FL</th>\n",
       "      <th>THDV_FL</th>\n",
       "      <th>THDE_FL</th>\n",
       "      <th>THDM_FL</th>\n",
       "      <th>THDR_FL</th>\n",
       "      <th>THDP_FL</th>\n",
       "      <th>THDOO_FL</th>\n",
       "      <th>Gender</th>\n",
       "      <th>cortical_ACA_ctr</th>\n",
       "      <th>cortical_MCA_ctr</th>\n",
       "      <th>subcortical_ACA_ctr</th>\n",
       "      <th>subcortical_MCA_ctr</th>\n",
       "      <th>PCA_cortex_ctr</th>\n",
       "      <th>thalamus_ctr</th>\n",
       "      <th>brainstem_ctr</th>\n",
       "      <th>cerebellum_ctr</th>\n",
       "      <th>Watershed_ctr</th>\n",
       "      <th>Hemorrhagic_infarct_ctr</th>\n",
       "      <th>cortical_ACA_ctl</th>\n",
       "      <th>cortical_MCA_ctl</th>\n",
       "      <th>subcortical_ACA_ctl</th>\n",
       "      <th>subcortical_MCA_ctl</th>\n",
       "      <th>PCA_cortex_ctl</th>\n",
       "      <th>thalamus_ctl</th>\n",
       "      <th>brainstem_ctl</th>\n",
       "      <th>cerebellum_ctl</th>\n",
       "      <th>Watershed_ctl</th>\n",
       "      <th>Hemorrhagic_infarct_ctl</th>\n",
       "      <th>NIHS_1a_in</th>\n",
       "      <th>NIHS_1b_in</th>\n",
       "      <th>NIHS_1c_in</th>\n",
       "      <th>NIHS_2_in</th>\n",
       "      <th>NIHS_3_in</th>\n",
       "      <th>NIHS_4_in</th>\n",
       "      <th>NIHS_5aL_in</th>\n",
       "      <th>NIHS_5bR_in</th>\n",
       "      <th>NIHS_6aL_in</th>\n",
       "      <th>NIHS_6bR_in</th>\n",
       "      <th>NIHS_7_in</th>\n",
       "      <th>NIHS_8_in</th>\n",
       "      <th>NIHS_9_in</th>\n",
       "      <th>NIHS_10_in</th>\n",
       "      <th>NIHS_11_in</th>\n",
       "      <th>elapsed_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>536.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>891</th>\n",
       "      <td>37.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>36.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>37.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>320.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>36.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>37.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>896 rows  63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BT_NM  HR_NM  RR_NM  HB_NM  HCT_NM  PLATELET_NM  WBC_NM  PTT1_NM  \\\n",
       "0     37.0   54.0   11.0   10.0    42.0         97.0    17.0     31.0   \n",
       "1     37.0  101.0   23.0   12.0    37.0        179.0     8.0     34.0   \n",
       "2     35.0   63.0   14.0   11.0    39.0        536.0     3.0     26.0   \n",
       "3     35.0   61.0   15.0   15.0    42.0        283.0    11.0     30.0   \n",
       "4     36.0   79.0   16.0   14.0    27.0        308.0     9.0     28.0   \n",
       "..     ...    ...    ...    ...     ...          ...     ...      ...   \n",
       "891   37.0   91.0   12.0   16.0    54.0         85.0    12.0     27.0   \n",
       "892   36.0   59.0   23.0   13.0    29.0         81.0     9.0     23.0   \n",
       "893   37.0  113.0   13.0   13.0    41.0        320.0     5.0     33.0   \n",
       "894   36.0  158.0   16.0   10.0    26.0        133.0     3.0     21.0   \n",
       "895   37.0  117.0   13.0   13.0    44.0        206.0     8.0     22.0   \n",
       "\n",
       "     PTT2_NM  PTINR_NM  ER_NM  BUN_NM  CRE_NM   BMI   age    PPD  THDA_FL  \\\n",
       "0       19.0       1.0  359.0    30.0     2.0  20.0  90.0  118.0      0.0   \n",
       "1       32.0       1.0  252.0    15.0     2.0  22.0  66.0   75.0      0.0   \n",
       "2       46.0       1.0  138.0    19.0     3.0  29.0  59.0   72.0      0.0   \n",
       "3       21.0       1.0  194.0    12.0     1.0  22.0  57.0   48.0      0.0   \n",
       "4       18.0       1.0  148.0    22.0     0.0  26.0  67.0   45.0      1.0   \n",
       "..       ...       ...    ...     ...     ...   ...   ...    ...      ...   \n",
       "891     32.0       1.0   96.0    17.0     0.0  24.0  60.0   49.0      0.0   \n",
       "892     21.0       2.0  192.0    26.0     1.0  20.0  71.0   37.0      1.0   \n",
       "893     64.0       1.0  129.0    13.0     1.0  22.0  70.0   64.0      0.0   \n",
       "894     40.0       1.0  140.0    17.0     1.0  18.0  93.0  106.0      1.0   \n",
       "895     23.0       1.0  135.0    22.0     0.0  26.0  55.0   69.0      0.0   \n",
       "\n",
       "     THDH_FL  THDI_FL  THDAM_FL  THDV_FL  THDE_FL  THDM_FL  THDR_FL  THDP_FL  \\\n",
       "0        0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1        0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2        0.0      1.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3        0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4        0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "..       ...      ...       ...      ...      ...      ...      ...      ...   \n",
       "891      0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "892      0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "893      0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "894      1.0      0.0       0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "895      0.0      0.0       0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "     THDOO_FL  Gender  cortical_ACA_ctr  cortical_MCA_ctr  \\\n",
       "0         0.0     1.0               0.0               0.0   \n",
       "1         0.0     1.0               0.0               0.0   \n",
       "2         0.0     1.0               0.0               0.0   \n",
       "3         1.0     1.0               0.0               0.0   \n",
       "4         0.0     0.0               0.0               0.0   \n",
       "..        ...     ...               ...               ...   \n",
       "891       0.0     1.0               0.0               0.0   \n",
       "892       0.0     1.0               0.0               1.0   \n",
       "893       0.0     0.0               0.0               0.0   \n",
       "894       0.0     0.0               0.0               0.0   \n",
       "895       0.0     0.0               0.0               0.0   \n",
       "\n",
       "     subcortical_ACA_ctr  subcortical_MCA_ctr  PCA_cortex_ctr  thalamus_ctr  \\\n",
       "0                    0.0                  0.0             0.0           1.0   \n",
       "1                    0.0                  0.0             0.0           0.0   \n",
       "2                    0.0                  0.0             0.0           0.0   \n",
       "3                    0.0                  0.0             0.0           0.0   \n",
       "4                    0.0                  0.0             0.0           0.0   \n",
       "..                   ...                  ...             ...           ...   \n",
       "891                  0.0                  0.0             0.0           0.0   \n",
       "892                  0.0                  1.0             0.0           0.0   \n",
       "893                  0.0                  0.0             0.0           0.0   \n",
       "894                  0.0                  1.0             0.0           0.0   \n",
       "895                  0.0                  0.0             0.0           0.0   \n",
       "\n",
       "     brainstem_ctr  cerebellum_ctr  Watershed_ctr  Hemorrhagic_infarct_ctr  \\\n",
       "0              0.0             0.0            0.0                      0.0   \n",
       "1              0.0             0.0            0.0                      0.0   \n",
       "2              0.0             0.0            0.0                      0.0   \n",
       "3              0.0             0.0            0.0                      0.0   \n",
       "4              0.0             0.0            0.0                      0.0   \n",
       "..             ...             ...            ...                      ...   \n",
       "891            0.0             0.0            0.0                      0.0   \n",
       "892            0.0             0.0            0.0                      0.0   \n",
       "893            0.0             0.0            0.0                      0.0   \n",
       "894            0.0             0.0            0.0                      0.0   \n",
       "895            0.0             0.0            0.0                      0.0   \n",
       "\n",
       "     cortical_ACA_ctl  cortical_MCA_ctl  subcortical_ACA_ctl  \\\n",
       "0                 0.0               0.0                  0.0   \n",
       "1                 0.0               0.0                  0.0   \n",
       "2                 0.0               0.0                  0.0   \n",
       "3                 0.0               0.0                  0.0   \n",
       "4                 0.0               0.0                  0.0   \n",
       "..                ...               ...                  ...   \n",
       "891               0.0               0.0                  0.0   \n",
       "892               0.0               0.0                  0.0   \n",
       "893               0.0               0.0                  0.0   \n",
       "894               0.0               0.0                  0.0   \n",
       "895               0.0               0.0                  0.0   \n",
       "\n",
       "     subcortical_MCA_ctl  PCA_cortex_ctl  thalamus_ctl  brainstem_ctl  \\\n",
       "0                    0.0             0.0           0.0            0.0   \n",
       "1                    0.0             0.0           0.0            0.0   \n",
       "2                    1.0             0.0           0.0            0.0   \n",
       "3                    0.0             0.0           0.0            0.0   \n",
       "4                    0.0             0.0           0.0            0.0   \n",
       "..                   ...             ...           ...            ...   \n",
       "891                  0.0             0.0           0.0            0.0   \n",
       "892                  0.0             0.0           0.0            0.0   \n",
       "893                  0.0             0.0           0.0            0.0   \n",
       "894                  0.0             0.0           0.0            0.0   \n",
       "895                  0.0             0.0           0.0            0.0   \n",
       "\n",
       "     cerebellum_ctl  Watershed_ctl  Hemorrhagic_infarct_ctl  NIHS_1a_in  \\\n",
       "0               0.0            0.0                      0.0         1.0   \n",
       "1               0.0            0.0                      0.0         0.0   \n",
       "2               0.0            0.0                      0.0         0.0   \n",
       "3               0.0            0.0                      0.0         0.0   \n",
       "4               0.0            0.0                      0.0         0.0   \n",
       "..              ...            ...                      ...         ...   \n",
       "891             0.0            0.0                      0.0         0.0   \n",
       "892             0.0            0.0                      0.0         1.0   \n",
       "893             0.0            0.0                      0.0         0.0   \n",
       "894             0.0            0.0                      0.0         0.0   \n",
       "895             0.0            0.0                      0.0         0.0   \n",
       "\n",
       "     NIHS_1b_in  NIHS_1c_in  NIHS_2_in  NIHS_3_in  NIHS_4_in  NIHS_5aL_in  \\\n",
       "0           0.0         0.0        0.0        0.0        1.0          0.0   \n",
       "1           0.0         0.0        0.0        1.0        0.0          0.0   \n",
       "2           0.0         0.0        0.0        0.0        0.0          0.0   \n",
       "3           0.0         0.0        0.0        0.0        1.0          2.0   \n",
       "4           0.0         0.0        2.0        2.0        2.0          4.0   \n",
       "..          ...         ...        ...        ...        ...          ...   \n",
       "891         2.0         1.0        0.0        0.0        1.0          0.0   \n",
       "892         0.0         0.0        2.0        2.0        2.0          4.0   \n",
       "893         0.0         0.0        0.0        0.0        0.0          0.0   \n",
       "894         0.0         0.0        0.0        0.0        0.0          0.0   \n",
       "895         0.0         0.0        0.0        0.0        2.0          4.0   \n",
       "\n",
       "     NIHS_5bR_in  NIHS_6aL_in  NIHS_6bR_in  NIHS_7_in  NIHS_8_in  NIHS_9_in  \\\n",
       "0            0.0          2.0          0.0        0.0        1.0        0.0   \n",
       "1            0.0          0.0          0.0        0.0        0.0        0.0   \n",
       "2            0.0          0.0          0.0        1.0        1.0        0.0   \n",
       "3            0.0          2.0          0.0        0.0        0.0        0.0   \n",
       "4            0.0          4.0          0.0        0.0        1.0        0.0   \n",
       "..           ...          ...          ...        ...        ...        ...   \n",
       "891          3.0          0.0          0.0        0.0        1.0        3.0   \n",
       "892          0.0          4.0          0.0        0.0        1.0        0.0   \n",
       "893          0.0          0.0          0.0        0.0        0.0        1.0   \n",
       "894          0.0          0.0          0.0        0.0        1.0        0.0   \n",
       "895          0.0          4.0          0.0        0.0        0.0        0.0   \n",
       "\n",
       "     NIHS_10_in  NIHS_11_in  elapsed_class  \n",
       "0           1.0         0.0            0.0  \n",
       "1           0.0         0.0            0.0  \n",
       "2           0.0         1.0            1.0  \n",
       "3           0.0         0.0            0.0  \n",
       "4           1.0         2.0            0.0  \n",
       "..          ...         ...            ...  \n",
       "891         2.0         0.0            0.0  \n",
       "892         1.0         1.0            1.0  \n",
       "893         1.0         0.0            1.0  \n",
       "894         0.0         0.0            1.0  \n",
       "895         2.0         0.0            1.0  \n",
       "\n",
       "[896 rows x 63 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dataset = pd.DataFrame(np.round(sc.inverse_transform(arr)), columns=[\n",
    "    'BT_NM', 'HR_NM', 'RR_NM', 'HB_NM', 'HCT_NM', 'PLATELET_NM', 'WBC_NM',\n",
    "    'PTT1_NM', 'PTT2_NM', 'PTINR_NM', 'ER_NM', 'BUN_NM', 'CRE_NM', 'BMI',\n",
    "    'age', 'PPD', 'THDA_FL', 'THDH_FL', 'THDI_FL', 'THDAM_FL', 'THDV_FL',\n",
    "    'THDE_FL', 'THDM_FL', 'THDR_FL', 'THDP_FL', 'THDOO_FL', 'Gender',\n",
    "    'cortical_ACA_ctr', 'cortical_MCA_ctr', 'subcortical_ACA_ctr',\n",
    "    'subcortical_MCA_ctr', 'PCA_cortex_ctr', 'thalamus_ctr',\n",
    "    'brainstem_ctr', 'cerebellum_ctr', 'Watershed_ctr',\n",
    "    'Hemorrhagic_infarct_ctr', 'cortical_ACA_ctl', 'cortical_MCA_ctl',\n",
    "    'subcortical_ACA_ctl', 'subcortical_MCA_ctl', 'PCA_cortex_ctl',\n",
    "    'thalamus_ctl', 'brainstem_ctl', 'cerebellum_ctl', 'Watershed_ctl',\n",
    "    'Hemorrhagic_infarct_ctl', 'NIHS_1a_in', 'NIHS_1b_in', 'NIHS_1c_in',\n",
    "    'NIHS_2_in', 'NIHS_3_in', 'NIHS_4_in', 'NIHS_5aL_in', 'NIHS_5bR_in',\n",
    "    'NIHS_6aL_in', 'NIHS_6bR_in', 'NIHS_7_in', 'NIHS_8_in', 'NIHS_9_in',\n",
    "    'NIHS_10_in', 'NIHS_11_in','elapsed_class'\n",
    "])\n",
    "output_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataset.to_csv(\"../dataset/output_dataset/ADS-GAN-constraint_models.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_TEST_dataset = pd.DataFrame(np.round(sc.inverse_transform(X_test)), columns=[\n",
    "    'BT_NM', 'HR_NM', 'RR_NM', 'HB_NM', 'HCT_NM', 'PLATELET_NM', 'WBC_NM',\n",
    "    'PTT1_NM', 'PTT2_NM', 'PTINR_NM', 'ER_NM', 'BUN_NM', 'CRE_NM', 'BMI',\n",
    "    'age', 'PPD', 'THDA_FL', 'THDH_FL', 'THDI_FL', 'THDAM_FL', 'THDV_FL',\n",
    "    'THDE_FL', 'THDM_FL', 'THDR_FL', 'THDP_FL', 'THDOO_FL', 'Gender',\n",
    "    'cortical_ACA_ctr', 'cortical_MCA_ctr', 'subcortical_ACA_ctr',\n",
    "    'subcortical_MCA_ctr', 'PCA_cortex_ctr', 'thalamus_ctr',\n",
    "    'brainstem_ctr', 'cerebellum_ctr', 'Watershed_ctr',\n",
    "    'Hemorrhagic_infarct_ctr', 'cortical_ACA_ctl', 'cortical_MCA_ctl',\n",
    "    'subcortical_ACA_ctl', 'subcortical_MCA_ctl', 'PCA_cortex_ctl',\n",
    "    'thalamus_ctl', 'brainstem_ctl', 'cerebellum_ctl', 'Watershed_ctl',\n",
    "    'Hemorrhagic_infarct_ctl', 'NIHS_1a_in', 'NIHS_1b_in', 'NIHS_1c_in',\n",
    "    'NIHS_2_in', 'NIHS_3_in', 'NIHS_4_in', 'NIHS_5aL_in', 'NIHS_5bR_in',\n",
    "    'NIHS_6aL_in', 'NIHS_6bR_in', 'NIHS_7_in', 'NIHS_8_in', 'NIHS_9_in',\n",
    "    'NIHS_10_in', 'NIHS_11_in','elapsed_class'\n",
    "])\n",
    "X_TEST_dataset.to_csv(\"../dataset/output_dataset/ADS-GAN-constraint_XTEST.csv\",encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## release the memory of gpu\n",
    "tf.keras.backend.clear_session()\n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
