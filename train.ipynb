{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import os, sys, time\n",
    "import util\n",
    "from model import Generator, Discriminator\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import layers\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=\"0\"\n",
    "print(tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"./dataset/df_noOutliner_ana.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, imp_mode, imp_mean=util.FeatureArrange(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "df = sc.fit_transform(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting hyperparameter\n",
    "latent_dim = 73\n",
    "epochs = 1\n",
    "batch_size= 128\n",
    "buffer_size = 6000\n",
    "# save_interval = 50\n",
    "n_critic = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_opt = tf.keras.optimizers.Adam(0.0001, 0.5, 0.9)\n",
    "disc_opt = tf.keras.optimizers.Adam(0.0001, 0.5, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset=tf.data.Dataset.from_tensor_slices(df.astype('float32'))\\\n",
    "    .shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x):\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:\n",
    "        gen_data = generator(noise, training=True)\n",
    "        gen_output = discriminator(gen_data, training=True)\n",
    "        real_output = discriminator(x, training=True )\n",
    "        \n",
    "#         print(\"gen_output:\",gen_output)\n",
    "#         print(\"real_output:\",real_output)\n",
    "        \n",
    "        x_hat = util.random_weight_average(x, gen_data)\n",
    "        d_hat = discriminator(x_hat,)\n",
    "#         print(\"x_hat:\",x_hat)\n",
    "#         print(\"d_hat:\",d_hat)\n",
    "        \n",
    "        disc_loss = util.discriminator_loss(real_output, gen_output, d_hat, x_hat)\n",
    "#         print(\"disc_loss:\",disc_loss)\n",
    "        gen_loss = util.generator_loss(gen_output)\n",
    "#         print(\"gen_loss:\",gen_loss)\n",
    "        \n",
    "    grad_disc = dis_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    grad_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    \n",
    "    disc_opt.apply_gradients(zip(grad_disc, discriminator.trainable_variables))\n",
    "    gen_opt.apply_gradients(zip(grad_gen, generator.trainable_variables))\n",
    "    return gen_output, real_output,x_hat,d_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_discriminator(x):\n",
    "    \n",
    "    noise = tf.random.normal([x.shape[0], latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as dis_tape:\n",
    "        gen_data = generator(noise, training=True)\n",
    "#         print(\"gen_data\",gen_data)\n",
    "        gen_output = discriminator(gen_data,training=True)\n",
    "#         print(\"gen_output\",gen_output)\n",
    "        real_output = discriminator(x, training=True)\n",
    "#         print(\"real_output\",real_output)\n",
    "        x_hat = util.random_weight_average(x, gen_data)\n",
    "        d_hat = discriminator(x_hat,)\n",
    "        \n",
    "        disc_loss = util.discriminator_loss(real_output, gen_output, d_hat, x_hat)\n",
    "    grad_disc = dis_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    disc_opt.apply_gradients(zip(grad_disc, discriminator.trainable_variables))\n",
    "\n",
    "    return disc_loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_generator():\n",
    "    noise = tf.random.normal([batch_size, latent_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        gen_data = generator(noise, training=True )\n",
    "        gen_output = discriminator(gen_data,training=True)\n",
    "\n",
    "        gen_loss = util.generator_loss(gen_output)\n",
    "\n",
    "    grad_gen = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(grad_gen, generator.trainable_variables))\n",
    "\n",
    "    return gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    disc_loss = 0\n",
    "    gen_loss = 0\n",
    "    \n",
    "    for data in training_dataset:\n",
    "#         gen_output, real_output,x_hat,d_hat=train_step(data)\n",
    "        disc_loss += train_discriminator(data)\n",
    "#         print(\"gen_output:\",gen_output)\n",
    "#         print(\"real_output\", real_output,)\n",
    "#         print(\"x_hat\", x_hat)\n",
    "#         print(\"d_hat\",d_hat)\n",
    "    \n",
    "        if disc_opt.iterations.numpy() % n_critic == 0:\n",
    "            gen_loss += train_generator()\n",
    "    print('Time for epoch {} is {} sec - gen_loss = {}, disc_loss = {}'.format(epoch + 1,\n",
    "                                                                               time.time() - start, gen_loss / batch_size, disc_loss / (batch_size*n_critic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disc_opt.iterations.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
